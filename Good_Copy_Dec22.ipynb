{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopy\n",
    "from geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim(user_agent=\"example app\")\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check for NaN values in our columns\n",
      "Place: False\n",
      "Potential Exposure Window: False\n",
      "Address: True\n",
      "Covid Exposure Or Precaution\t: False\n",
      "Zone: False\n",
      "Last Updated: False\n",
      "There are no NaN values\n",
      "Geolocation Address Column Completed\n",
      "Creating First Batch of Geocoded Addresses, this might take a while...\n",
      "Completed First Batch\n",
      "Creating Second Batch of Geocoded Addresses, attempt to geocode the ones that failed\n",
      "Complete\n",
      "Drop Remaining rows with NaN, can be cleaned up later/\n",
      "Exporting to CSV\n",
      "CSV Complete\n"
     ]
    }
   ],
   "source": [
    "# URL\n",
    "URL = \"https://www.nshealth.ca/coronavirus-exposures\"\n",
    "\n",
    "# Get content\n",
    "content = requests.get(URL)\n",
    "\n",
    "# Parse\n",
    "soup = BeautifulSoup(content.text, 'html.parser')\n",
    "\n",
    "# Find last page class tag\n",
    "last_page = soup.find(\"li\", {\"class\": \"pager__item pager__item--last\"})\n",
    "\n",
    "# Find last page link\n",
    "last_page_link = last_page.find_all(href=True)\n",
    "\n",
    "# Turn link to string\n",
    "last_page_link_str = str(last_page_link)\n",
    "\n",
    "# Find the page number in the link string\n",
    "last_page_num = last_page_link_str.split(\"page=\")\n",
    "\n",
    "# Turn the page number into an integer\n",
    "last_page_n = int(last_page_num[1][0])\n",
    "\n",
    "\n",
    "# Create an empty dictionary to store dataframes\n",
    "df_dict = {}\n",
    "\n",
    "# Add page 1 as the first(0) element in the dictionary\n",
    "df_dict[0] = pd.read_html(\"http://www.nshealth.ca/covid-exposures\")[0]\n",
    "\n",
    "# Loop through the URL for later pages, up until the last page\n",
    "# Enumerate new dictionary elements based on page number\n",
    "for i in range(1, last_page_n):\n",
    "    df = pd.read_html(\"https://www.nshealth.ca/coronavirus-exposures?title=&field_covid_exposure_zone_value=All&page={}\".format(i))[0]\n",
    "    df_dict[i] = df\n",
    "    \n",
    "# Create a final dataframe\n",
    "df_final = pd.concat(df_dict.values(), ignore_index=True)  \n",
    "\n",
    "\n",
    "#########################Manual Changes#########################\n",
    "\n",
    "spider = df_final[df_final[\"Place\"].str.contains(\"Spider-Man\")].index\n",
    "df_final.loc[spider,\"Address\"] = \"Cineplex Sydney\"\n",
    "####Change time structure for Last Updated column############\n",
    "df_final[\"Last Updated\"] = df_final[\"Last Updated\"].str.split(\"-\",expand=True)[0].str.split(\"/\")[2][2].strip(\" \") \\\n",
    "+\"-\"+ df_final[\"Last Updated\"].str.split(\"-\",expand=True)[0].str.split(\"/\")[2][0] \\\n",
    " +\"-\"+ df_final[\"Last Updated\"].str.split(\"-\",expand=True)[0].str.split(\"/\")[2][1] \\\n",
    " +\" \"+ df_final[\"Last Updated\"].str.split(\"-\",expand=True)[1].str.strip(\" \")\n",
    "\n",
    "\n",
    "#################################################################\n",
    "#####Check the dataframe to see which columns have NaN values####\n",
    "#################################################################\n",
    "if df_final.isnull().values.any():\n",
    "    print(\"Check for NaN values in our columns\")\n",
    "    print(\"Place: \" + str(df_final[\"Place\"].isnull().values.any()))\n",
    "    print(\"Potential Exposure Window: \" + str(df_final[\"Potential Exposure Window\"].isnull().values.any()))\n",
    "    print(\"Address: \" + str(df_final[\"Address\"].isnull().values.any()))\n",
    "    print(\"Covid Exposure Or Precaution\t: \" + str(df_final[\"Covid Exposure Or Precaution\"].isnull().values.any()))\n",
    "    print(\"Zone: \" + str(df_final[\"Zone\"].isnull().values.any()))\n",
    "    print(\"Last Updated: \" + str(df_final[\"Last Updated\"].isnull().values.any()))\n",
    "\n",
    "##Determine Which Addresses Show up As Null    \n",
    "#Clean them by replacing the address \n",
    "null_addresses = df_final[df_final[\"Address\"].isnull()].index\n",
    "for null_address in null_addresses:\n",
    "    #watch out for the \"-\" which NS health uses ... wtf\n",
    "    df_final.iloc[null_address,2] = df_final.iloc[null_address][0].split(\"â€“\")[1]\n",
    "    \n",
    "#################################################################\n",
    "#####Check the dataframe to see which columns have NaN values####\n",
    "#################################################################\n",
    "if df_final.isnull().values.any():\n",
    "    print(\"Check for NaN values in our columns\")\n",
    "    print(\"Place: \" + str(df_final[\"Place\"].isnull().values.any()))\n",
    "    print(\"Potential Exposure Window: \" + str(df_final[\"Potential Exposure Window\"].isnull().values.any()))\n",
    "    print(\"Address: \" + str(df_final[\"Address\"].isnull().values.any()))\n",
    "    print(\"Covid Exposure Or Precaution\t: \" + str(df_final[\"Covid Exposure Or Precaution\"].isnull().values.any()))\n",
    "    print(\"Zone: \" + str(df_final[\"Zone\"].isnull().values.any()))\n",
    "    print(\"Last Updated: \" + str(df_final[\"Last Updated\"].isnull().values.any()))\n",
    "else:\n",
    "    print(\"There are no NaN values\")\n",
    "   \n",
    "\n",
    "df_final[\"Exposure_From\"] = df_final[\"Potential Exposure Window\"]\\\n",
    "                                    .str.split(\"to\",expand=True)[0]\\\n",
    "                                    .str.split(\",\",expand=True)[1]\\\n",
    "                                    .str.split(\"-\",expand=True)[0]\\\n",
    "                                    .str.split(\"/\",expand=True)[2].str.strip(\" \")\\\n",
    "  +\"-\"+df_final[\"Potential Exposure Window\"]\\\n",
    "                                    .str.split(\"to\",expand=True)[0]\\\n",
    "                                    .str.split(\",\",expand=True)[1]\\\n",
    "                                    .str.split(\"-\",expand=True)[0]\\\n",
    "                                    .str.split(\"/\",expand=True)[0].str.strip(\" \")\\\n",
    "  +\"-\"+df_final[\"Potential Exposure Window\"]\\\n",
    "                                    .str.split(\"to\",expand=True)[0]\\\n",
    "                                    .str.split(\",\",expand=True)[1]\\\n",
    "                                    .str.split(\"-\",expand=True)[0]\\\n",
    "                                    .str.split(\"/\",expand=True)[1].str.strip(\" \")\\\n",
    "  +\" \"+ df_final[\"Potential Exposure Window\"].str.split(\"to\",expand=True)[0].str.split(\",\",expand=True)[1].str.split(\"-\",expand=True)[1].str.strip(\" \")\n",
    "\n",
    "#Select days which have exposures in a single day\n",
    "one_day = df_final[\"Potential Exposure Window\"].str.split(\"to\",expand=True)[1].str.len() <=7\n",
    "df_final.loc[one_day,\"Exposure_To\"] = df_final[\"Potential Exposure Window\"]\\\n",
    "                                    .str.split(\"to\",expand=True)[0]\\\n",
    "                                    .str.split(\",\",expand=True)[1]\\\n",
    "                                    .str.split(\"-\",expand=True)[0]\\\n",
    "                                    .str.split(\"/\",expand=True)[2].str.strip(\" \")\\\n",
    "  +\"-\"+df_final[\"Potential Exposure Window\"]\\\n",
    "                                    .str.split(\"to\",expand=True)[0]\\\n",
    "                                    .str.split(\",\",expand=True)[1]\\\n",
    "                                    .str.split(\"-\",expand=True)[0]\\\n",
    "                                    .str.split(\"/\",expand=True)[0].str.strip(\" \")\\\n",
    "  +\"-\"+df_final[\"Potential Exposure Window\"]\\\n",
    "                                    .str.split(\"to\",expand=True)[0]\\\n",
    "                                    .str.split(\",\",expand=True)[1]\\\n",
    "                                    .str.split(\"-\",expand=True)[0]\\\n",
    "                                    .str.split(\"/\",expand=True)[1].str.strip(\" \")\\\n",
    "  +\" \"+ df_final[\"Potential Exposure Window\"].str.split(\"to\",expand=True)[0].str.split(\",\",expand=True)[1].str.split(\"-\",expand=True)[1].str.strip(\" \")\n",
    "\n",
    " \n",
    "#Select days that have longer exposure periods\n",
    "two_days = df_final[\"Potential Exposure Window\"].str.split(\"to\",expand=True)[1].str.len() >=7\n",
    "df_final.loc[two_days,\"Exposure_To\"] = df_final[\"Potential Exposure Window\"]\\\n",
    "                                    .str.split(\"to\",expand=True)[1]\\\n",
    "                                    .str.split(\",\",expand=True)[1]\\\n",
    "                                    .str.split(\"-\",expand=True)[0]\\\n",
    "                                    .str.split(\"/\",expand=True)[2].str.strip(\" \")\\\n",
    "  +\"-\"+df_final[\"Potential Exposure Window\"]\\\n",
    "                                    .str.split(\"to\",expand=True)[1]\\\n",
    "                                    .str.split(\",\",expand=True)[1]\\\n",
    "                                    .str.split(\"-\",expand=True)[0]\\\n",
    "                                    .str.split(\"/\",expand=True)[0].str.strip(\" \")\\\n",
    "  +\"-\"+df_final[\"Potential Exposure Window\"]\\\n",
    "                                    .str.split(\"to\",expand=True)[1]\\\n",
    "                                    .str.split(\",\",expand=True)[1]\\\n",
    "                                    .str.split(\"-\",expand=True)[0]\\\n",
    "                                    .str.split(\"/\",expand=True)[1].str.strip(\" \")\\\n",
    "  +\" \"+ df_final[\"Potential Exposure Window\"].str.split(\"to\",expand=True)[1].str.split(\",\",expand=True)[1].str.split(\"-\",expand=True)[1].str.strip(\" \")\n",
    "\n",
    " \n",
    "#Create a Geolocation Address column\n",
    "df_final['GeolocationAddress'] = df_final['Address'] + \", Nova Scotia, Canada\" \n",
    "print(\"Geolocation Address Column Completed\")\n",
    "print(\"Creating First Batch of Geocoded Addresses, this might take a while...\")\n",
    "df_final['GeolocationAddressCoords'] = df_final['GeolocationAddress'].apply(lambda x: geolocator.geocode(x))\n",
    "properly_geocoded = df_final.dropna(subset = [\"GeolocationAddressCoords\"])\n",
    "improperly_geocoded = df_final[~df_final.index.isin(properly_geocoded.index)]\n",
    "df_final[\"Latitude\"] = properly_geocoded[\"GeolocationAddressCoords\"].apply(lambda x: (x.latitude))\n",
    "df_final[\"Longitude\"] = properly_geocoded[\"GeolocationAddressCoords\"].apply(lambda x: (x.longitude))\n",
    "print(\"Completed First Batch\")\n",
    "\n",
    "\n",
    "#Second Fix\n",
    "print(\"Creating Second Batch of Geocoded Addresses, attempt to geocode the ones that failed\")\n",
    "df_final.loc[improperly_geocoded.index,\"GeolocationAddressCoords\"] = (improperly_geocoded[\"Place\"] + \", Nova Scotia, Canada\").apply(lambda x: geolocator.geocode(x))\n",
    "properly_geocoded_2 = df_final.dropna(subset = [\"GeolocationAddressCoords\"])\n",
    "improperly_geocoded_2 = df_final[~df_final.index.isin(properly_geocoded_2.index)]\n",
    "df_final[\"Latitude\"] = properly_geocoded_2[\"GeolocationAddressCoords\"].apply(lambda x: (x.latitude))\n",
    "df_final[\"Longitude\"] = properly_geocoded_2[\"GeolocationAddressCoords\"].apply(lambda x: (x.longitude))\n",
    "print(\"Complete\")\n",
    "\n",
    "print(\"Drop Remaining rows with NaN, can be cleaned up later/\")\n",
    "\n",
    "#CREATE FINAL DATA SET WITH NA ROWS REMOVED\n",
    "df_final_2 = df_final.dropna()\n",
    "df_final_2.reset_index(drop=True,inplace=True)\n",
    "df_for_valerie = df_final_2[[\"Place\",\"Exposure_From\",\"Exposure_To\",\"GeolocationAddress\",\"Covid Exposure Or Precaution\",\"Zone\",\"Last Updated\",\"Latitude\",\"Longitude\"]]\n",
    "df_for_valerie.astype(str)\n",
    "\n",
    "\n",
    "print(\"Exporting to CSV\")\n",
    "df_for_valerie.to_csv('./covidlocations.csv', header=False)\n",
    "print(\"CSV Complete\")\n",
    "\n",
    "\n",
    "val2 = df_for_valerie\n",
    "val2.update('\"' +df_final_2[[\"Place\",\"Exposure_From\",\"Exposure_To\",\"GeolocationAddress\",\"Covid Exposure Or Precaution\",\"Zone\",\"Last Updated\",\"Latitude\",\"Longitude\"]].astype(str) + '\"' )\n",
    "val2.to_csv('./covidlocationsstrings.csv', header=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting to CSV\n",
      "CSV Complete\n"
     ]
    }
   ],
   "source": [
    "print(\"Exporting to CSV\")\n",
    "df_final_2.to_csv('./covidlocations.csv', header=False)\n",
    "print(\"CSV Complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
